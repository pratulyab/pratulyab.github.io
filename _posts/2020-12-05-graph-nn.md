---
layout: distill
title: Graph Neural Networks and Graph Convolution
description: A no-brainer primer on graph neural networks and graph convolution to get a quick sense about their workings
keywords: 
date: 2020-12-05 00:00:00-0000
author: Pratulya Bubna
comments: true
bibliography: 2020-12-05-graph-nn.bib
og_image: /assets/img/blog/gnn/gnn.png

---
This post is intended to provide a very high-level intuition about the workings of Graph Convolutional Networks (GCNs)in the spatial domain. The goal is to provide an overall rough-idea about GNNs and GCNs without delving into the meaty math.

- [Introduction](#intro)
- [Graph Convolutional Networks (GCNs)](#gcn)
	- [Graph Convolution](#graphconv)
    - [Recipe](#recipe)
	- [All in an equation](#gcn-eqn)

<div class="row row-cols-1 mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <img class="img-fluid rounded z-depth-1" src="{{ site.baseurl }}/assets/img/blog/gnn/gnn.png">
    </div>
</div>
<div class="caption">
    An illustration of a graph-based network. (image credits <d-cite key="guo2020deep"></d-cite>)
</div>

<!-- Introduction -->
<a name="intro"></a>
### Introduction
Images, text and audio data can easily be represented as a grid &mdash; a regular and structured format. Given this representation, Convolutional Neural Networks (CNNs) can be successfully applied to process this data. 

However, _non-Euclidean_ data such as social networks, 3D meshes, molecular data, knowledge graphs &mdash; inherently graphs, cannot be represented in a grid-like structure and instead lie in an irregular domain. Consequently, the convolution operation in the _Euclidean_ case (CNNs) is not well-defined on non-Euclidean domains.

The learning architecture designed to handle unstructured, non-Euclidean graph data is **Graph Neural Networks (GNNs)**. GNNs **do not have strict structural requirements** as opposed to regular neural networks that operate on fixed-dimension inputs (eg. CNNs built over MNIST dataset require all input images to be of size 28x28). This means that the number of vertices and edges between input graphs can change.

Variants of GNNs are **Graph Convolutional Networks (GCNs)** that have evolved due to works in generalizing convolutions to the graph domain.

<div class="l-body"><hr style="margin:0;"></div>

A graph is a simple way of encoding pairwise relationships among a set of objects.
> A graph $$G$$ consists of a pair of sets $$(V, E)$$ &mdash; a collection $$V$$ of `nodes` and a collection of $$E$$ of `edges`. Each edge $$ e \in E $$ joins two nodes and thus is a two-element subset of $$V: e = \{u, v\}$$ for some $$u, v \in V$$.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <img class="img-fluid rounded z-depth-1" src="{{ site.baseurl }}/assets/img/blog/gnn/graph.jpg">
    </div>
</div>
<div class="caption">
    Graph (credits <d-footnote><a href="https://youtu.be/eCDBA_SbxCE">Ravi, Nikhila: "3D Deep Learning with PyTorch3D <d-cite key="ravi2020pytorch3d"></d-cite>", ICML 2020"</a></d-footnote>)
</div>


<!-- Graph Convolutional Networks -->
<a name="gcn"></a>
### Graph Convolutional Networks
CNNs learn features **hierarchically** by building from simpler ones to more complex. Desirable properties of CNNs include weight sharing, local connectivity, non-linearity and pooling layers, that have helped them achieve outstanding performance in a variety of tasks.

In graphs, the notion of neighborhoods and connectivity is **ill-defined**, making the application of convolution and pooling operations _non-trivial_. Two approaches to tackle this: _spectral_ and _non-spectral_. Whereas spectral approaches deal with spectral representations of graphs (eg. graph Laplacian), non-spectral approaches define convolutions directly on the graph.

That is, the **spatial approaches** assume that graphs are embedded in a Euclidean space where nodes can have coordinates and weights on them. This assumption helps in generalizing pixels (in case of CNNs) to nodes of the graph, over which we can take the similar _sliding window_ approach to aggregate the features of the neighborhoods.


<!-- Graph Conv -->
<a name="graphconv"></a>
### Graph Convolution
We'll stick to GCNs in spatial domain, in which, convolution is usually implemented through non-linear functions (eg. MLP) over spatial neighbors, and pooling may be adopted to produce a new **coarsened** graph by aggregating information from each point's neighbors.


#### Message Passing and Neighborhood Aggregation
<div class="row row-cols-1 mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <img class="img-fluid rounded z-depth-1" src="{{ site.baseurl }}/assets/img/blog/gnn/message-passing.gif">
    </div>
</div>
<div class="caption">
    A simple illustration. (credits <d-footnote><a href="https://youtu.be/2KRAOZIULzw">Zak Jost: Graph Convolutional Networks"</a></d-footnote>)
</div>

We can choose to determine a fix-sized (node degree) or a variable neighborhood, which would require to define an operator that can work with **different sized neighborhoods**. After determining the neighborhood, an aggregator is performed over it, for instance, `mean` over all the neighbor's feature vectors.

The aggregation operator generally is chosen to be **permutation invariant**. That is, the aggregated output is invariant to the order of neighbors picked. The usual choices being `sum`, `mean` and `max`.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <img class="img-fluid rounded z-depth-1" src="{{ site.baseurl }}/assets/img/blog/gnn/neighborhood-agg.gif">
    </div>
</div>
<div class="caption">
    An illustration of graph convolution. (credits <d-footnote><a href="https://youtu.be/2KRAOZIULzw">Zak Jost: Graph Convolutional Networks"</a></d-footnote>)
</div>


<div class="l-body"><hr style="margin:0;"></div>

<!-- Recipe -->
<a name="recipe"></a>
### Graph Convolution: Recipe
<div class="row row-cols-4 mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <img class="img-fluid rounded z-depth-1" src="{{ site.baseurl }}/assets/img/blog/gnn/1.jpg">
    </div>
    <div class="col-sm mt-3 mt-md-0">
        <img class="img-fluid rounded z-depth-1" src="{{ site.baseurl }}/assets/img/blog/gnn/2.jpg">
    </div>
    <div class="col-sm mt-3 mt-md-0">
        <img class="img-fluid rounded z-depth-1" src="{{ site.baseurl }}/assets/img/blog/gnn/3.jpg">
    </div>
    <div class="col-sm mt-3 mt-md-0">
        <img class="img-fluid rounded z-depth-1" src="{{ site.baseurl }}/assets/img/blog/gnn/4.jpg">
    </div>
</div>
<div class="caption">
    (left-right 1&ndash;4) 4-step recipe. (credits <d-footnote><a href="https://youtu.be/eCDBA_SbxCE">Ravi, Nikhila: "3D Deep Learning with PyTorch3D <d-cite key="ravi2020pytorch3d"></d-cite>"", ICML 2020"</a></d-footnote>)
</div>

1. Select a vertex
2. Find its neighboring vertices using the edges
3. Aggregate (eg. sum) features of the neighbors
4. Update features of the selected node


<div class="l-body"><hr style="margin:0;"></div>

<!-- Equation -->
<a name="gcn-eqn"></a>
### GCNs in an equation

The equation encapsulates message passing and neighborhood aggregation in GCNs.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
    $$
	\mathbf{x}_i^{(k)} = \gamma^{(k)} \biggl( \mathbf{x}_i^{(k-1)} , \square_{j \in \mathcal{N}(i)} \quad  \phi^{(k)} \Bigl( \mathbf{x}_i^{(k-1)} , \mathbf{x}_j^{(k-1)} , \mathbf{e}_{j,i}  \Bigr) \biggr) \quad
	\text{where,}
	$$
	$$ \mathbf{x}_i^{(k)} \in \mathbb{R}^{F'} $$
	$$ \mathbf{x}_i^{(k-1)}, \mathbf{x}_j^{(k-1)} \in \mathbb{R}^{F} $$
	$$ \mathbf{e}_{j,i} \in \mathbb{R}^{D} $$
    </div>
</div>

<div class="l-body"><hr></div>

#### in words

Calculating features of node $$\mathbf{x}_i$$ in $$ k^{th} $$ layer is a non-linear function of:
1. its features in previous layer $$ i.e. \, \mathbf{x}_i^{(k-1)} $$; and,
2. aggregegated output over its neighbors $$ j \in \mathcal{N}(i) $$
	-  each "output" is a non-linear function of $$ \mathbf{x}_i^{(k-1)}, $$ neighbor features $$ \mathbf{x}_j^{(k-1)} $$ and (optionally) of the edge features $$ \mathbf{e}_{j,i} $$ between them (all in the previous layer $$ k-1 $$)

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <img class="img-fluid rounded z-depth-1" src="{{ site.baseurl }}/assets/img/blog/gnn/eqn.png">
    </div>
</div>
<div class="caption">
    Detailed equation (credits: <d-footnote><a href="https://pytorch-geometric.readthedocs.io/en/latest/notes/create_gnn.html">PyTorch Geometric</a></d-footnote>)
</div>


***
:bulb: ```TODO: add citations``` <br>
:loudspeaker: ``` the discussion section is below``` [:arrow_down:](#disqus_thread) 


